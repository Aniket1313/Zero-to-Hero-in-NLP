{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h4>Unit 2 <h1 style=\"text-align:center\"> Chapter 2</h1>\n",
    " \n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity\n",
    "\n",
    "* A score to measure the performance of a language model.\n",
    "* In practice we cannot use raw probabilities, hence we use a variant of probability called <strong>perplexity(PP)</strong>.\n",
    "\n",
    "> Perplexity of a language model on a test set is the inverse probability of a test set, normalized by words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a test set having $N$ words $W = (w_1, w_2, w_3,...w_n)$\n",
    "\n",
    "$PP(W) = P(w_1, w_2, w_3,.., w_n)^-\\frac{1}{N}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Since it's a inverse metric, higher the conditional probability of a word, lower the perplexity.\n",
    "\n",
    "Thus minimizing perplexity is equivalent to maximizing test set probability.\n",
    "\n",
    "In other words, <strong>lower the perplexity, better it is.</strong>\n",
    "\n",
    "Hence, PP score for a bigram model would be:\n",
    "\n",
    "![Bigram Perplexity](../images/bigram_perplexity.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Another look at Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also think of perplexity as the <strong>weighted average branching factor of a language</strong>.\n",
    "\n",
    "> The branching factor is the number of possible words that can follow any word in a language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the task of recognizing the digits in English (zero, one, two,..., nine), given that (both in some training set and in some test set) each of the 10 digits occurs with equal probability P = 1/10. The perplexity of this mini-language is in fact 10. To see that, imagine a test string of digits of length N, and assume that in the training set all the digits occurred with equal probability. \n",
    "\n",
    "\n",
    "But suppose that the number zero is really frequent and occurs far more often than other numbers. Let’s say that 0 occur 91 times in the training set, and each of the other digits occurred 1 time each. Now we see the following test set: 0 0 0 0 0 3 0 0 0 0. We should expect the perplexity of this test set to be lower since most of the time the next number will be zero, which is very predictable, i.e. has a high probability. Thus, although the branching factor is still 10, the perplexity or weighted branching factor is smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<strong>The perplexity of two language models can be calculated only if they use identical vocabularies.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some important points\n",
    "\n",
    "* n-grams do a better and better job of modeling the training corpus as we increase the value of N.\n",
    "* The longer the context on which we train the model, the more coherent the sentences. \n",
    "\n",
    "> Coherent - Consistent\n",
    "\n",
    "* Statistical models are likely to be pretty useless as predictors if the training sets and the test sets are different.\n",
    "\n",
    "Example, train a language model on Shakespeare & test it on another prominent poet Milton.\n",
    "\n",
    "To solve this problem, use a training corpus that has a similar genre to whatever task we are trying to accomplish.\n",
    "\n",
    "Example, to build a translator for legal documents, train it on legal documents.\n",
    "\n",
    "* If we have many zero probability n-grams, then it affects the language model as well.\n",
    "\n",
    "For example, suppose our training set has the following sentences and their frequencies,\n",
    "\n",
    "denied the allegations: 5 <br>\n",
    "deniedthespeculation: 2 <br>\n",
    "denied the rumors: 1 <br>\n",
    "denied the report: 1 <br>\n",
    "\n",
    "Now, suppose the test set has a sentence,\n",
    "\n",
    "'denied the loan', the language model will assign 0 probability to this trigram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since perplexity is inverse probability of test set, and if some words have 0 probability, then the perplexity of the test set is 0. Hence we can't calculate perplexity at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "How to handle this condition — A word is not present in the training set, but present in test set, as a result it's assigned a probability of 0.\n",
    "\n",
    "#### Continued in next chapter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
