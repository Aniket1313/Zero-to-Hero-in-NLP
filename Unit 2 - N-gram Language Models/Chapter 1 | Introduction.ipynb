{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h4>Unit 2 <h1 style=\"text-align:center\"> Chapter 1</h1>\n",
    " \n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NLP, some tasks like, speech recognition, autocorrect, requires to predict the next word. \n",
    "\n",
    "Suppose you have a sentence \"I am going to take a ......\"\n",
    "\n",
    "You must have predicted the word \"walk\" or \"bath\" to complete the sentence as \"I am going to take a walk\".\n",
    "\n",
    "But why not \"talk\" or \"dance\" ? Because judging from the previous sentences it's probable that the next word would be walk, or bath, or shower.\n",
    "\n",
    "---\n",
    "\n",
    "That's what a language model does. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language Models (LMs)\n",
    "\n",
    "> A language model assigns probability to each possible next word. The same models can be used to assign probability over an entire sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For writing tools like spelling correction or grammatical error correction, we need to find and correct errors in writing like Their are two midterms, in which There was mistyped as Their, or Everything has improve, in which improve should have been improved. The phrase There are will be much more probable than Their are, and has improved\n",
    "than has improve.\n",
    "\n",
    "---\n",
    "\n",
    "N-gram are the simplest language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-grams\n",
    "\n",
    "> A N-gram is a sequence of <strong>N</strong> words.\n",
    "\n",
    "For example,\n",
    "\n",
    "\"good day\" is a bi-gram/2-gram.\n",
    "\n",
    "\"keep moving straight\" is a tri-gram/3-gram.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's represent probability(P) of a word(w) given some history(h) is ```P(w|h)```.\n",
    "\n",
    "Suppose the word is <strong>walk</strong> and  the history is <strong>I am goin to take a</strong>.\n",
    "\n",
    "So, we can represent this as, the probability of the word <strong>walk</strong> given the history <strong>I am going to take a </strong>is :\n",
    "\n",
    "<h4 style=\"text-align:center\">$P(w \\mid h)$</h4>\n",
    "\n",
    "or \n",
    "\n",
    "<h4 style=\"text-align:center\">$P(\"walk\" \\mid \"I\\space am\\space going\\space to\\space take\\space a\")$</h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One method of finding this probability is to use frequency counts.\n",
    "\n",
    "Count the number of times we see \"I am going to take a\" & count the number of times we see \"I am going to take a walk\" in a large corpus.\n",
    "\n",
    "\n",
    "<h4 style=\"text-align:center\">$P(\"walk\" \\mid \"I\\space am\\space going\\space to\\space take\\space a\")= \\dfrac{C(\"I\\space am\\space going\\space to\\space take\\space a\\space walk\")}{C(\"I\\space am\\space going\\space to\\space take\\space a\")}$ </h4>\n",
    "\n",
    "where, \n",
    "\n",
    "C = count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This method works fine but since language changes frequently, the new sentences that come up, won't be present in any large corpus, hence we will end up with poor probability estimates.\n",
    "\n",
    "We will be seeing cleverer ways ahead to compute these probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notations,\n",
    "\n",
    "- Sequence of n words = $w_1, w_2, w_3,.....w_n$\n",
    "- Joint probability of each word in a sequence having particular value = $P(X=w_1,Y=w_2,Z=w_3,...W=w_n)$ = $P(w_1, w_2, w_3,....,w_n)$\n",
    "\n",
    "---\n",
    "\n",
    "To compute probability of entire sequences like $P(w_1, w_2, w_3,....,w_n)$, we can use chain rule of probability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(w^n_1 ) = P(w_1 )P(w_2 |w_1 )P(w_3 |w^2_1 ) . . . P(w_n |w_1^{n−1} )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The intuition of the n-gram model is that instead of computing the probability of\n",
    "a word given its entire history, we can approximate the history by just the last few\n",
    "words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bi-gram\n",
    "\n",
    "> The bi-gram model approximates the probability of a word given all the previous words by using only the conditional probability of the preceding word.\n",
    "\n",
    "Example, if using a bi-gram model, instead of computing the probability,\n",
    "\n",
    "<h4 style=\"text-align:center\">$P(\"walk\" \\mid \"I\\space am\\space going\\space to\\space take\\space a\")$</h4>\n",
    "\n",
    "we approximate it with the probablity,\n",
    "\n",
    "<h4 style=\"text-align:center\">$P(\"walk\" \\mid \"a\")$</h4>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-grams are the simplest, oldest language models, which looks (n − 1) words into the past to assign probability to next word.\n",
    "\n",
    "Similarly, \n",
    "\n",
    "A bi-gram(2-gram), looks (2-1) = 1 word in the past to predict next word,\n",
    "\n",
    "A tri-gram(3-gram), looks (3-1) =2 words in the past to predict next word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assumption that the probability of a word depends on only the previous word, is called <strong>Markov</strong> assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
