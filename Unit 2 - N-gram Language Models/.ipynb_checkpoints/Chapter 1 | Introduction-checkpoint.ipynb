{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h4>Unit 2 <h1 style=\"text-align:center\"> Chapter 1</h1>\n",
    " \n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NLP, some tasks like, speech recognition, autocorrect, requires to predict the next word. \n",
    "\n",
    "Suppose you have a sentence \"I am going to take a ......\"\n",
    "\n",
    "You must have predicted the word \"walk\" or \"bath\" to complete the sentence as \"I am going to take a walk\".\n",
    "\n",
    "But why not \"talk\" or \"dance\" ? Because judging from the previous sentences it's probable that the next word would be walk, or bath, or shower.\n",
    "\n",
    "---\n",
    "\n",
    "That's what a language model does. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language Models (LMs)\n",
    "\n",
    "> A language model assigns probability to each possible next word. The same models can be used to assign probability over an entire sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For writing tools like spelling correction or grammatical error correction, we need to find and correct errors in writing like Their are two midterms, in which There was mistyped as Their, or Everything has improve, in which improve should have been improved. The phrase There are will be much more probable than Their are, and has improved\n",
    "than has improve.\n",
    "\n",
    "---\n",
    "\n",
    "N-gram are the simplest language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-grams\n",
    "\n",
    "> A N-gram is a sequence of <strong>N</strong> words.\n",
    "\n",
    "For example,\n",
    "\n",
    "\"good day\" is a bi-gram/2-gram.\n",
    "\n",
    "\"keep moving straight\" is a tri-gram/3-gram.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's represent probability(P) of a word(w) given some history(h) is ```P(w|h)```.\n",
    "\n",
    "Suppose the word is <strong>walk</strong> and  the history is <strong>I am goin to take a</strong>.\n",
    "\n",
    "So, we can represent this as, the probability of the word <strong>walk</strong> given the history <strong>I am going to take a </strong>is :\n",
    "\n",
    "<h4 style=\"text-align:center\">$P(w \\mid h)$</h4>\n",
    "\n",
    "or \n",
    "\n",
    "<h4 style=\"text-align:center\">$P(\"walk\" \\mid \"I\\space am\\space going\\space to\\space take\\space a\")$</h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One method of finding this probability is to use frequency counts.\n",
    "\n",
    "Count the number of times we see \"I am going to take a\" & count the number of times we see \"I am going to take a walk\" in a large corpus.\n",
    "\n",
    "\n",
    "<h4 style=\"text-align:center\">$P(\"walk\" \\mid \"I\\space am\\space going\\space to\\space take\\space a\")= \\dfrac{C(\"I\\space am\\space going\\space to\\space take\\space a\\space walk\")}{C(\"I\\space am\\space going\\space to\\space take\\space a\")}$ </h4>\n",
    "\n",
    "where, \n",
    "\n",
    "C = count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This method works fine but since language changes frequently, the new sentences that come up, won't be present in any large corpus, hence we will end up with poor probability estimates.\n",
    "\n",
    "We will be seeing cleverer ways ahead to compute these probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notations,\n",
    "\n",
    "- Sequence of n words = $w_1, w_2, w_3,.....w_n$\n",
    "- Joint probability of each word in a sequence having particular value = $P(X=w_1,Y=w_2,Z=w_3,...W=w_n)$ = $P(w_1, w_2, w_3,....,w_n)$\n",
    "\n",
    "---\n",
    "\n",
    "To compute probability of entire sequences like $P(w_1, w_2, w_3,....,w_n)$, we can use chain rule of probability.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(w^n_1 ) = P(w_1 )P(w_2 |w_1 )P(w_3 |w^2_1 ) . . . P(w_n |w_1^{n−1} )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The intuition of the n-gram model is that instead of computing the probability of\n",
    "a word given its entire history, we can approximate the history by just the last few\n",
    "words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bi-gram\n",
    "\n",
    "> The bi-gram model approximates the probability of a word given all the previous words by using only the conditional probability of the preceding word.\n",
    "\n",
    "Example, if using a bi-gram model, instead of computing the probability,\n",
    "\n",
    "<h4 style=\"text-align:center\">$P(\"walk\" \\mid \"I\\space am\\space going\\space to\\space take\\space a\")$</h4>\n",
    "\n",
    "we approximate it with the probablity,\n",
    "\n",
    "<h4 style=\"text-align:center\">$P(\"walk\" \\mid \"a\")$</h4>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-grams are the simplest, oldest language models, which looks (n − 1) words into the past to assign probability to next word.\n",
    "\n",
    "Similarly, \n",
    "\n",
    "A bi-gram(2-gram), looks (2-1) = 1 word in the past to predict next word,\n",
    "\n",
    "A tri-gram(3-gram), looks (3-1) =2 words in the past to predict next word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assumption that the probability of a word depends on only the previous word, is called <strong>Markov</strong> assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brush up your concepts of probability from [here](https://stats.libretexts.org/Bookshelves/Introductory_Statistics/Book%3A_Introductory_Statistics_(Shafer_and_Zhang)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To estimate the probabilities of these bi-gram & tri-gram, we can use <strong>Maximum Likelihood Estimation</strong>, where we get the MLE estimate of the parameters of an n-gram model by getting counts from the corpus and normalizing to have them in the range 0 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, to compute a particular bigram probability of a word y given a previous word x, we’ll compute the count of the bigram C(xy) and normalize by the sum of all the bigrams that share the same first word x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example\n",
    "\n",
    "Let's look at the corpus, D, have three sentences.\n",
    "\n",
    "D {\n",
    "\n",
    "s1 = 'I am Sam',\n",
    "\n",
    "s2 = 'Sam I am',\n",
    "\n",
    "s3 = 'I do not like green eggs and ham'\n",
    "\n",
    "}\n",
    "\n",
    "> We will augment these sentences to mark the end and start of the sentences.\n",
    "\n",
    "So now the corpus looks like,\n",
    "\n",
    "D {\n",
    "\n",
    "s1 = '[s] I am Sam [/s],\n",
    "\n",
    "s2 = '[s] Sam I am [/s]',\n",
    "\n",
    "s3 = '[s] I do not like green eggs and ham [/s]'\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now let's calculate some of the bigram probabilities,\n",
    "\n",
    "Suppose you want to find the probability of 'I' at the start of a sentence (based on this corpus):\n",
    "\n",
    "We'll use the notation  $P(I\\space|\\space[s])$, this notation says that, what is the probability of <strong>I</strong> as the start <strong>[s]</strong>.\n",
    "\n",
    "In other words, we need to find the probability of bigram, '[s] I'.\n",
    "\n",
    "Now, let's count.\n",
    "\n",
    "' [s] I ' are encountered in 2 sentences out of 3 in the corpus, hence $P(I\\space|\\space[s]) = \\frac{2}{3} $\n",
    "\n",
    "Similarly, \n",
    "\n",
    "$P(Sam\\space|\\space[s]) = \\frac{1}{3} $\n",
    "\n",
    "$P(Sam\\space|\\space am) = \\frac{1}{2} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying enough n-grams, and the number would be too low. Hence we use log probabilities.\n",
    "\n",
    "Multipliying in linear space is equal to addition in log space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Language Models\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Extransic evaluation.\n",
    "\n",
    "> Embed the language model into your application and see end-to-end how it improves the system.\n",
    "\n",
    "2. Intrinsic evaluation.\n",
    "\n",
    "> Like every other statistical model the estimated probabilities of the language model comes from training set, we can test on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
