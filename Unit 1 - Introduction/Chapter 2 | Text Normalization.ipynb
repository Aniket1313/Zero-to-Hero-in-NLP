{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h4>Unit 1 <h1 style=\"text-align:center\"> Chapter 2</h1>\n",
    " \n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Words\n",
    "---\n",
    "\n",
    "Take a look at this sentence :\n",
    "\n",
    "'The quick brown fox jumps over the lazy fox, and took his meal.'\n",
    "\n",
    "* The sentence has 13 _Words_ if you don't count punctuations, and 15 if you count punctions. \n",
    "\n",
    "* To count punctuation as a word or not depends on the task in hand.\n",
    "\n",
    "* For some tasks like P-O-S tagging & speech synthesis, punctuations are treated as words. (Hello! and Hello? are different in speech synthesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('The quick brown fox jumps over the lazy fox, and took his meal.'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utterance\n",
    "\n",
    "> An utterance is a spoken correlate of a sentence. (Speaking a sentence is utterance)\n",
    "\n",
    "Take a look at this sentence:\n",
    "\n",
    "'I am goi- going to the market to buy ummm fruits.'\n",
    "\n",
    "* This utterance has two kinds of <strong>disfluencies</strong>(disorder in smooth flow).\n",
    "\n",
    "1. Fragment - The broken off word 'goi' is a fragment.\n",
    "2. Fillers - Words like ummm, uhhh, are called fillers or filled pauses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lemma\n",
    "\n",
    "> A lemma is a set of lexical forms having the same stem, the same major part-of-speech, and the same word sense.\n",
    "\n",
    "* Wordform is the full inflected or derived form of the word.\n",
    "\n",
    "Example,\n",
    "\n",
    "Wordforms - cats,cat\n",
    "\n",
    "Lemma - cat\n",
    "\n",
    "Wordforms - Moving, move\n",
    "\n",
    "Lemma - move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vocabulary, Wordtypes, and Wordtokens\n",
    "\n",
    "* Vocabulary - It is the set of distinct words in a corpus.\n",
    "\n",
    "* Wordtypes - It is the size of the vocabulary V i.e. |V|\n",
    "\n",
    "* Wordtokens - It is the total number of running words.\n",
    "\n",
    "Take a look at this sentence:\n",
    "\n",
    "'They picnicked by the pool, then lay back on the grass and looked at the stars.'\n",
    "\n",
    "Here,\n",
    "\n",
    "* Vocabulary = V = {They, picnicked, by, the, pool, then, lay, back, on, grass, and, looked, at, stars}\n",
    "\n",
    "* Wordtypes = |V| = 14\n",
    "\n",
    "* Wordtokens(ignoring punctuation) = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocTypeToken(sentence):\n",
    "    tokens = sentence.split()\n",
    "    vocabulary = list(set(tokens))\n",
    "    wordtypes = len(vocabulary)\n",
    "    wordtokens = len(tokens)\n",
    "    print(\"Sentence = {}\\n\".format(sentence))\n",
    "    print(\"Tokens = {}\\n\".format(tokens))\n",
    "    print(\"Vocabulary = {}\\n\".format(sorted(vocabulary)))\n",
    "    print(\"Wordtypes = {}\\n\".format(wordtypes))\n",
    "    print(\"Wordtokens = {}\".format(wordtokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence = They picnicked by the pool, then lay back on the grass and looked at the stars.\n",
      "\n",
      "Tokens = ['They', 'picnicked', 'by', 'the', 'pool,', 'then', 'lay', 'back', 'on', 'the', 'grass', 'and', 'looked', 'at', 'the', 'stars.']\n",
      "\n",
      "Vocabulary = ['They', 'and', 'at', 'back', 'by', 'grass', 'lay', 'looked', 'on', 'picnicked', 'pool,', 'stars.', 'the', 'then']\n",
      "\n",
      "Wordtypes = 14\n",
      "\n",
      "Wordtokens = 16\n"
     ]
    }
   ],
   "source": [
    "sentence = 'They picnicked by the pool, then lay back on the grass and looked at the stars.'\n",
    "vocTypeToken(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Herdan's Law\n",
    "\n",
    "> The larger the corpora we look at, the more wordtypes we find. The relationsip between wordtypes and tokens is called <strong>Herdan's Law</strong>\n",
    "\n",
    "\\begin{equation*}\n",
    "|V| = kN^\\beta \n",
    "\\end{equation*}\n",
    ", k and \\\\(\\beta\\\\) are positive consonants.\n",
    "\n",
    "The value of \\\\(\\beta\\\\) depends on the corpus size and is in the range of 0 to 1.\n",
    "\n",
    "* We can say that the vocabulary size for a text goes up significantly faster than the square root of its length in words.\n",
    "---\n",
    "\n",
    "- Another rough measure of number of words in a corpus is the number of lemmas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Code switching\n",
    "\n",
    "> The phenonmenon of changing lanugage while reading or writing is called code switching.\n",
    "\n",
    "Example,\n",
    "\n",
    "'Tu mera dost hai or rahega, don't worry.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Normalization\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before any type of natural language processing, the text has to be brought a normal condition or state.\n",
    "\n",
    "The below mentioned three tasks are common for almost every normalization process.\n",
    "\n",
    "1. Tokenizing ( breaking into words )\n",
    "2. Normalizing word formats\n",
    "3. Segmenting sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Word tokenization\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The task of segmenting text into words.\n",
    "\n",
    "<p style=\"color:red\">Why you should not use split() for tokenizaiton.</p>\n",
    "\n",
    "If using split() on the text, the words like 'Mr. Randolf', emails like 'hello@internet.com' may be broken down as ['Mr.','Randolf'], emails may be broken down as ['hello','@','internet','.','com'].\n",
    "\n",
    "This is not what we generally want, hence special tokenization algorithms must be used.\n",
    "\n",
    "* Commas are generally used as word boundaries but also in large numbers (540,000).\n",
    "* Periods are generally used as sentence boundaries but also in emails, urls, salutation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clitic\n",
    "\n",
    "> Clitics are words that can't stand on their own. They are attached to other words. Tokenizer can be used to expand clitics.\n",
    "\n",
    "Example of clitics,\n",
    "\n",
    "What're, Who's, You're.\n",
    "\n",
    "\n",
    "- Tokenization algorithms can also be used to tokenize multiwords like 'New York', 'rock N roll'.\n",
    "\n",
    "This tokenization is used in conjunction with <strong>Named Entity Detection</strong> (the task of detecting name, places, dates, organizations)\n",
    "\n",
    "\n",
    "Python code for tokenization below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'The San Francisco-based restaurant,\" they said, \"doesn’t charge $10\".'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'San', 'Francisco-based', 'restaurant', ',', \"''\", 'they', 'said', ',', '``', 'doesn', '’', 't', 'charge', '$', '10', \"''\", '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'San', 'Francisco', '-', 'based', 'restaurant', ',\"', 'they', 'said', ',', '\"', 'doesn', '’', 't', 'charge', '$', '10', '\".']\n"
     ]
    }
   ],
   "source": [
    "print(wordpunct_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since tokenization needs to run before any language processing, it needs to be fast.\n",
    "\n",
    "Regex based tokenization is fast but not that smart while handling punctuations, and language dilemma. \n",
    "There are many tokenization algorithms like ByteLevelBPETokenizer, CharBPETokenizer, SentencePieceBPETokenizer.\n",
    "\n",
    "Below excercise shows step by step guide to modern way of tokenization using [huggingface's](https://huggingface.co/) ultrafast tokenization library - [Tokenizer](https://github.com/huggingface/tokenizers)\n",
    "                             \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notice the speed of huggingface tokenizer and nltk tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in /opt/anaconda3/lib/python3.7/site-packages (0.7.0)\r\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install tokenizers #install tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (BertWordPieceTokenizer)\n",
    "tokenizer = BertWordPieceTokenizer(\"bert-base-uncased-vocab.txt\", lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def textTokenizer(text):\n",
    "    start = (datetime.now())\n",
    "    print(tokenizer.encode(text).tokens)\n",
    "    end = (datetime.now())\n",
    "    print(\"Time taken - {}\".format((end-start).total_seconds()))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'number', 'expressions', 'introduce', 'other', 'complications', 'as', 'well', ';', 'while', 'com', '##mas', 'nor', '-', 'mall', '##y', 'appear', 'at', 'word', 'boundaries', ',', 'com', '##mas', 'are', 'used', 'inside', 'numbers', 'in', 'english', ',', 'every', 'three', 'digits', '.', '[SEP]']\n",
      "Time taken - 0.00062\n"
     ]
    }
   ],
   "source": [
    "textTokenizer('Number expressions introduce other complications as well; while commas nor- mally appear at word boundaries, commas are used inside numbers in English, every three digits.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We will discuss about [CLS] and [SEP] later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def nltkTokenizer(text):\n",
    "    start = (datetime.now())\n",
    "    print(word_tokenize(text))\n",
    "    end = (datetime.now())\n",
    "    print(\"Time taken - {}\".format((end-start).total_seconds()))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Number', 'expressions', 'introduce', 'other', 'complications', 'as', 'well', ';', 'while', 'commas', 'nor-', 'mally', 'appear', 'at', 'word', 'boundaries', ',', 'commas', 'are', 'used', 'inside', 'numbers', 'in', 'English', ',', 'every', 'three', 'digits', '.']\n",
      "Time taken - 0.002232\n"
     ]
    }
   ],
   "source": [
    "nltkTokenizer('Number expressions introduce other complications as well; while commas nor- mally appear at word boundaries, commas are used inside numbers in English, every three digits.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word segmentation\n",
    "\n",
    "> Some languages(like Chinese) don't have words seperated by spaces, hence tokenization is not easily done. So word segmentation is done using sequence models trained on hand seperated datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
