{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h1><spanstyle=\"text-align:centre\">Chapter 3</span></h1>\n",
    " \n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far in the past notebooks, we've seen two methods on segmenting words from running texts.\n",
    "\n",
    "1. Tokenization using regular expression.\n",
    "2. Tokenization using word segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "There's a third approach of tokenization. Instead of defining tokens as word seperated by spaces or as characters, we can use our data to automatically tell what size a token must be.\n",
    "\n",
    "Example,\n",
    "\n",
    "Sometimes we want tokens to be space delimited, sometimes large word tokens (New York) and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Subword\n",
    "\n",
    "> A subword is in between a word and a character.\n",
    "\n",
    "Example,\n",
    "\n",
    "We can split the word 'subword' between 'sub' & 'word'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "If our training corpus contains, say the words low, and lowest, but not lower, but then the word lower appears in our test corpus, our system will not know what to do with it.\n",
    "A solution to this problem is to use a kind of tokenization in which most tokens are words, but some tokens are frequent morphemes or other subwords like -er, so that an unseen word can be represented by combining the parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Byte Pair Encoding for Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We start with a dictionary of word-frequency, and a vocabulary. {word:frequency}\n",
    "* End of word is marked by _\n",
    "\n",
    "Consider the following example,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary\n",
      "------\n",
      "low_ : 5\n",
      "lowest_ : 2\n",
      "newer_ : 6\n",
      "wider_ : 3\n",
      "new_ : 2\n",
      "\n",
      "Vocabulary\n",
      "------\n",
      "_ d e i l n o r s t w\n"
     ]
    }
   ],
   "source": [
    "dictionary = {'low_':5,'lowest_':2,'newer_':6,'wider_':3,'new_':2}\n",
    "print(\"Dictionary\\n------\")\n",
    "for i,j in zip(dictionary.keys(),dictionary.values()):\n",
    "    print(str(i)+' : '+str(j))\n",
    "    \n",
    "print(\"\\nVocabulary\\n------\")\n",
    "vocabulary = ['_', 'd', 'e', 'i', 'l', 'n', 'o', 'r', 's', 't', 'w']\n",
    "print(*vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most frequent pair of symbols is 'r_' because it occurs 3+6 = 9 times.\n",
    "We now merge it and treat it as a single symbol, and then count again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated vocabulary\n",
      "-------\n",
      "_ d e i l n o r s t w r_\n"
     ]
    }
   ],
   "source": [
    "print(\"Updated vocabulary\\n-------\")\n",
    "vocabulary.append('r_')\n",
    "print(*vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the most frequent pair is 'er_' , which we merge; our system has learned that there should be a token for word-final er, represented as 'er_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated vocabulary\n",
      "-------\n",
      "_ d e i l n o r s t w r_ er_\n"
     ]
    }
   ],
   "source": [
    "print(\"Updated vocabulary\\n-------\")\n",
    "vocabulary.append('er_')\n",
    "print(*vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the most frequent pair is 'ew' (8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated vocabulary\n",
      "-------\n",
      "_ d e i l n o r s t w r_ er_ ew\n"
     ]
    }
   ],
   "source": [
    "print(\"Updated vocabulary\\n-------\")\n",
    "vocabulary.append('ew')\n",
    "print(*vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This goes on until the merges are complete and our system learns a new vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated vocabulary\n",
      "-------\n",
      "_ d e i l n o r s t w r_ er_ ew new lo low newer_ low_\n"
     ]
    }
   ],
   "source": [
    "print(\"Updated vocabulary\\n-------\")\n",
    "vocabulary.extend(['new','lo','low','newer_' ,'low_'])\n",
    "print(*vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we need to tokenize a test sentence, we just run the merges we have learned, greedily, in the order we learned them, on the test data. (Thus the fre- quencies in the test data don’t play a role, just the frequencies in the training data). So first we segment each test sentence word into characters. Then we apply the first rule: replace every instance of r   in the test corpus with r   , and then the second rule: replace every instance of e r in the test corpus with er , and so on. By the end, if the test corpus contained the word n e w e r , it would be tokenized as a full word. But a new (unknown) word like l o w e r would be merged into the two tokens low er_ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('l', 'o')\n",
      "('lo', 'w')\n",
      "('low', '</w>')\n",
      "('low', 'e')\n",
      "('lowe', 's')\n",
      "('lowes', 't')\n",
      "('lowest', '</w>')\n",
      "('n', 'e')\n"
     ]
    }
   ],
   "source": [
    "import re , collections\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int) \n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word. split ()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs [ symbols [ i ] , symbols [ i +1]] \n",
    "    return pairs\n",
    "def mergevocab(pair, v_in): \n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)') \n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word] \n",
    "    return v_out\n",
    "vocab = {'l o w </w>' : 5, 'l o w e s t </w>' : 2, 'n e w e r </w> ':6, 'w i d e r </w>':3, 'n e w</w>':2}\n",
    "num_merges = 8\n",
    "for i in range(num_merges):\n",
    "    pairs = get_stats(vocab)\n",
    "    best = max( pairs , key=pairs.get ) \n",
    "    vocab = mergevocab( best , vocab ) \n",
    "    print(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many more algorithms to learn vocabularies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods of tokenization using python\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using [NLTK](https://www.nltk.org/)\n",
    "> NLTK - Nautural Language Toolkit is a suite of libraries to work with natural language data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'The town was fairly large with a dozen or\\\n",
    "            so business buildings on each side of the street but, as I said, most were closed.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'town', 'was', 'fairly', 'large', 'with', 'a', 'dozen', 'or', 'so', 'business', 'buildings', 'on', 'each', 'side', 'of', 'the', 'street', 'but', ',', 'as', 'I', 'said', ',', 'most', 'were', 'closed', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK word tokenizer is a simple and effective tokenizer but over the years there has been a huge improvement in NLP.\n",
    "The recent tokenizer by HuggingFace is better and faster than NLTK.\n",
    "\n",
    "Some of the key features of [huggingface](huggingface.co) tokenizers are:\n",
    "\n",
    "- Performance (“takes less than 20 seconds to tokenize a GB of text on a server’s CPU”)\n",
    "- Provides access to the latest tokenizers for research and production use cases (BPE/byte-level BPE/WordPiece/SentencePiece…)\n",
    "- Aims for ease-of-use and versatility\n",
    "- Offers reproducibility of the original text that corresponds to the token using alignments tracking\n",
    "- Applies pre-processing best practices such as truncating, padding, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "* Using [tokenizers](https://github.com/huggingface/tokenizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (BertWordPieceTokenizer)\n",
    "tokenizer = BertWordPieceTokenizer(\"bert-base-uncased-vocab.txt\", lowercase=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def textTokenizer(text):\n",
    "    start = (datetime.now())\n",
    "    print(tokenizer.encode(text).tokens)\n",
    "    end = (datetime.now())\n",
    "    print(\"Time taken - {}\".format((end-start).total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'the', 'town', 'was', 'fairly', 'large', 'with', 'a', 'dozen', 'or', 'so', 'business', 'buildings', 'on', 'each', 'side', 'of', 'the', 'street', 'but', ',', 'as', 'i', 'said', ',', 'most', 'were', 'closed', '.', '[SEP]']\n",
      "Time taken - 0.00049\n"
     ]
    }
   ],
   "source": [
    "textTokenizer(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "* Using [Spacy](https://spacy.io/api/tokenizer/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'town', 'was', 'fairly', 'large', 'with', 'a', 'dozen', 'or', '           ', 'so', 'business', 'buildings', 'on', 'each', 'side', 'of', 'the', 'street', 'but', ',', 'as', 'I', 'said', ',', 'most', 'were', 'closed', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = spacy_nlp(sentence)\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
